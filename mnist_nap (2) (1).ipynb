{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import onnx\n",
        "import json\n",
        "from onnx2pytorch import ConvertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2WNl2l9Gs0_q"
      },
      "outputs": [],
      "source": [
        "\n",
        "TRAIN_BS = 32\n",
        "TEST_BS = 32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "DLD_DATA = True\n",
        "train_set = datasets.MNIST('./data', train=True, download=DLD_DATA,\n",
        "                          transform=transform)\n",
        "test_set = datasets.MNIST('./data', train=False, download=DLD_DATA,\n",
        "                          transform=transform)\n",
        "X_train_tensor = train_set.data\n",
        "y_train_tensor = train_set.targets\n",
        "X_test_tensor = test_set.data\n",
        "y_test_tensor = test_set.targets\n",
        "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
        "num_labels = len(np.unique(test_set.targets))\n",
        "num_inputs = len(X_train_tensor[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DPvg1unT0DXK"
      },
      "outputs": [],
      "source": [
        "random_seed=42\n",
        "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "def get_relu_activations(model, input):\n",
        "  model_relu_layers=[]\n",
        "  i=0\n",
        "  for name, layer in model.named_modules():\n",
        "    if i>0:\n",
        "      input=layer(input)\n",
        "      if isinstance(layer, nn.ReLU):\n",
        "        model_relu_layers.append(input[0]) #if using unsqeeze, use [0].\n",
        "    i=i+1\n",
        "  concatenated_tensor=torch.concatenate(model_relu_layers)\n",
        "  concatenated_tensor.flatten()\n",
        "  return concatenated_tensor\n",
        "\n",
        "\n",
        "def get_binary_abstraction(activations):\n",
        "  nap=[]\n",
        "  for i in activations:\n",
        "    if i>0:\n",
        "      nap.append(1)\n",
        "    elif i==0:\n",
        "      nap.append(0)\n",
        "    else:\n",
        "      nap.append('*')\n",
        "  return nap\n",
        "\n",
        "\n",
        "def find_states(onnx_path=None, states_path=\"wbc_480_relu_states.txt\"):\n",
        "  global neurons, relu_layers\n",
        "\n",
        "  neurons=0\n",
        "  relu_layers=0\n",
        "  # Assuming the ReLU layers are named similarly to your PyTorch model, you can add hooks like this\n",
        "  for name, layer in model.named_modules():\n",
        "    if isinstance(layer, nn.Linear):\n",
        "      neurons=layer.in_features\n",
        "    if isinstance(layer, torch.nn.ReLU):\n",
        "      relu_layers+=1\n",
        "\n",
        "  states={}\n",
        "  for i in range(num_labels):\n",
        "    states[i]=[]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, t in train_loader:\n",
        "        t=t.item()\n",
        "        i=i.float().to(device)\n",
        "        out=get_relu_activations(model,i)\n",
        "        states[t].append(out[0].detach().cpu().numpy().tolist())\n",
        "\n",
        "  with open(states_path, \"w\") as fp:\n",
        "    json.dump(states, fp)\n",
        "\n",
        "  return states\n",
        "\n",
        "def get_label_naps(states,delta=0.99):\n",
        "  label_naps={}\n",
        "  for label in states:\n",
        "    label_naps[label]={}\n",
        "    for relu in range(len(states[label][0])):\n",
        "      if (np.count_nonzero(states[label][:,relu])/len(states[label][:,relu]))>=delta:\n",
        "        label_naps[label][relu]=1\n",
        "      elif (np.count_nonzero(states[label][:,relu])/len(states[label][:,relu])) <=(1-delta):\n",
        "        label_naps[label][relu]=0\n",
        "  return label_naps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1Kq72Vnu6Nf5"
      },
      "outputs": [],
      "source": [
        "model = ConvertModel(onnx.load('mnist-net_256x4.onnx')).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kp-u-xFmXK8",
        "outputId": "6585fe90-28a0-43a2-84f6-563b171a048a"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m       t\u001b[38;5;241m=\u001b[39mt\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     19\u001b[0m       i\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m       out\u001b[38;5;241m=\u001b[39m\u001b[43mget_relu_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m       states[t]\u001b[38;5;241m.\u001b[39mappend(out\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(states))\n",
            "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mget_relu_activations\u001b[0;34m(model, input)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mReLU):\n\u001b[1;32m     13\u001b[0m       model_relu_layers\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m#if using unsqeeze, use [0].\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# images, labels = next(iter(train_loader))\n",
        "# images\n",
        "neurons=0\n",
        "relu_layers=0\n",
        "# Assuming the ReLU layers are named similarly to your PyTorch model, you can add hooks like this\n",
        "for name, layer in model.named_modules():\n",
        "  if isinstance(layer, nn.Linear):\n",
        "    neurons=layer.in_features\n",
        "  if isinstance(layer, torch.nn.ReLU):\n",
        "    relu_layers+=1\n",
        "\n",
        "states={}\n",
        "for i in range(num_labels):\n",
        "  states[i]=[]\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, t in train_loader:\n",
        "      t=t.item()\n",
        "      i=i.float().to(device)\n",
        "      out=get_relu_activations(model,i)\n",
        "      states[t].append(out.detach().cpu().numpy().tolist())\n",
        "print(len(states))\n",
        "print(len(states[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xebqZwqz3Qzo"
      },
      "outputs": [],
      "source": [
        "for i in states:\n",
        "  states[i]=np.array(states[i])\n",
        "label_naps = get_label_naps(states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oFeoEHeAHki",
        "outputId": "127f2006-7383-47de-9d17-f618e9bc7465"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "neurons=0\n",
        "# Assuming the ReLU layers are named similarly to your PyTorch model, you can add hooks like this\n",
        "for name, layer in model.named_modules():\n",
        "  if isinstance(layer, nn.Linear):\n",
        "    neurons=layer.in_features\n",
        "  if isinstance(layer, torch.nn.ReLU):\n",
        "      i=i+1\n",
        "relu_layers=i"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
